{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import  SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    result = ''\n",
    "    while True:\n",
    "        try:\n",
    "            start = s.index(first) + len(first)\n",
    "            end = s.index( last, start )\n",
    "            print(start, end)\n",
    "            result += s[start:end] + ' '\n",
    "            print(result)\n",
    "            s = s[end:]\n",
    "            print('made it')\n",
    "        except ValueError:\n",
    "            return result\n",
    "\n",
    "def find_between_r( s, first, last ):\n",
    "    try:\n",
    "        start = s.rindex( first ) + len( first )\n",
    "        end = s.rindex( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "bush = find_between(line, '<BUSH:>', '<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'party': 'republican', 'war': 'war'}\n"
     ]
    }
   ],
   "source": [
    "parties = ['republican', 'democrat', 'democrat', 'republican', 'democrat', 'republican', 'republican', 'republican', 'democrat', 'democrat', 'republican', 'democrat', 'republican', 'democrat']\n",
    "pres = ['bush', 'carter', 'clinton', 'eisenhower', 'fdroosevelt', 'ford', 'gwbush', 'hoover', 'kennedy', 'lbjohnson', 'nixon', 'obama', 'reagan', 'truman']\n",
    "war = ['war', 'nw', 'nw', 'nw', 'war', 'nw', 'war', 'nw', 'war', 'war', 'war', 'war', 'nw', 'war']\n",
    "pres_dict = {}\n",
    "for i, v in enumerate(pres):\n",
    "    pres_dict[v]= {}\n",
    "    pres_dict[v]['party'] = parties[i]\n",
    "    pres_dict[v]['war'] = war[i]\n",
    "print(pres_dict['bush'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports .txt files and puts them into a dictionary\n",
    "import os\n",
    "data = []\n",
    "for i, p in enumerate(pres):\n",
    "    folder = 'corpus/' + p + '/'\n",
    "    for filename in os.listdir(folder): \n",
    "        indiv = []\n",
    "        indiv.append(p)\n",
    "        indiv.append(parties[i])\n",
    "        indiv.append(war[i])\n",
    "        path = folder + filename\n",
    "        if os.path.isfile(path) and filename.endswith(\".txt\"): \n",
    "            with open(path, \"r\") as file: \n",
    "                indiv.append(file.read())\n",
    "            data.append(indiv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the dictionary to a dataframe\n",
    "df = pd.DataFrame (data, columns = ['name', 'party', 'war','speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>party</th>\n",
       "      <th>war</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bush</td>\n",
       "      <td>republican</td>\n",
       "      <td>war</td>\n",
       "      <td>&lt;title=\"Address at West Point\"&gt;\\n&lt;date=\"Januar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bush</td>\n",
       "      <td>republican</td>\n",
       "      <td>war</td>\n",
       "      <td>&lt;title=\"Address on Somalia\"&gt;\\n&lt;date=\"December ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bush</td>\n",
       "      <td>republican</td>\n",
       "      <td>war</td>\n",
       "      <td>&lt;title=\"Address on Iraq's Invasion of Kuwait\"&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bush</td>\n",
       "      <td>republican</td>\n",
       "      <td>war</td>\n",
       "      <td>&lt;title=\"Address Before a Joint Session of Cong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bush</td>\n",
       "      <td>republican</td>\n",
       "      <td>war</td>\n",
       "      <td>&lt;title=\"Remarks at Texas A and M University\"&gt;\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name       party  war                                             speech\n",
       "0  bush  republican  war  <title=\"Address at West Point\">\\n<date=\"Januar...\n",
       "1  bush  republican  war  <title=\"Address on Somalia\">\\n<date=\"December ...\n",
       "2  bush  republican  war  <title=\"Address on Iraq's Invasion of Kuwait\">...\n",
       "3  bush  republican  war  <title=\"Address Before a Joint Session of Cong...\n",
       "4  bush  republican  war  <title=\"Remarks at Texas A and M University\">\\..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### splits the speech into title, date, text\n",
    "def get_text(x):\n",
    "    idx = x.find('>', x.find('>')+1, 100)\n",
    "    return x[idx+1:]\n",
    "\n",
    "df['title'] = df['speech'].apply(lambda x: x.split('\"')[1])\n",
    "df['date'] = df['speech'].apply(lambda x: x.split('\"')[3])\n",
    "df['text'] = df['speech'].apply(get_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ask for help with this part\n",
    "df['interview'] = df['interview' in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speeches by president lbjohnson      71\n",
      "reagan         59\n",
      "fdroosevelt    49\n",
      "obama          48\n",
      "kennedy        45\n",
      "gwbush         39\n",
      "clinton        39\n",
      "hoover         29\n",
      "bush           23\n",
      "nixon          23\n",
      "carter         22\n",
      "truman         19\n",
      "ford           14\n",
      "eisenhower      6\n",
      "Name: name, dtype: int64\n",
      "speeches by party democrat      293\n",
      "republican    193\n",
      "Name: party, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##EDA!!!\n",
    "\n",
    "print(f'Speeches by president {df.name.value_counts()}')\n",
    "print(f'speeches by party {df.party.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3/7 of presidents are classified as \"not war\" (Hoover, Eisenhower, Ford, Carter, Reagan, Clinton).  \n",
    "#These presidents have a disproportionate number of 1 term presidents, so their share of speeches are roughly\n",
    "#proportional to their time in office\n",
    "print(df['war'].value_counts())\n",
    "print(169/486, 3/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-03d893bc764d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mBoW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBoW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# %\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# problem with labels name in this box!!!\n",
    "%%\n",
    "count_vector = CountVectorizer(stop_words='english', max_features=50)\n",
    "X = count_vector.fit_transform(df.text)\n",
    "# %%\n",
    "BoW = pd.DataFrame(X.toarray(), columns=labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(BoW, df.party)\n",
    "# %\n",
    "labels = count_vector.get_feature_names()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "## Initial Multinomial Bayes' run\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "MultinomialNB()\n",
    "print(f'Accuracy: {clf.score(X_test, y_test)}')\n",
    "print(f'labels: {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['republican', 'democrat', 'democrat', 'democrat', 'democrat',\n",
      "       'democrat', 'republican', 'republican', 'democrat', 'republican',\n",
      "       'democrat', 'republican', 'democrat', 'democrat', 'democrat',\n",
      "       'democrat', 'democrat', 'republican', 'democrat', 'republican',\n",
      "       'democrat', 'democrat', 'democrat', 'democrat', 'democrat',\n",
      "       'democrat', 'republican', 'republican', 'republican', 'democrat',\n",
      "       'republican', 'democrat', 'republican', 'democrat', 'democrat',\n",
      "       'republican', 'democrat', 'democrat', 'democrat', 'democrat',\n",
      "       'democrat', 'democrat', 'republican', 'democrat', 'republican',\n",
      "       'republican', 'republican', 'democrat', 'republican', 'republican',\n",
      "       'republican', 'democrat', 'republican', 'republican', 'democrat',\n",
      "       'republican', 'democrat', 'republican', 'republican', 'democrat',\n",
      "       'republican', 'democrat', 'republican', 'republican', 'republican',\n",
      "       'republican', 'republican', 'republican', 'republican',\n",
      "       'republican', 'democrat', 'republican', 'republican', 'republican',\n",
      "       'republican', 'democrat', 'republican', 'republican', 'democrat',\n",
      "       'republican', 'republican', 'republican', 'republican',\n",
      "       'republican', 'democrat', 'republican', 'republican', 'republican',\n",
      "       'republican', 'republican', 'democrat', 'democrat', 'republican',\n",
      "       'republican', 'republican', 'democrat', 'republican', 'democrat',\n",
      "       'democrat', 'republican', 'republican', 'democrat', 'democrat',\n",
      "       'democrat', 'democrat', 'republican', 'democrat', 'democrat',\n",
      "       'democrat', 'republican', 'republican', 'republican', 'democrat',\n",
      "       'democrat', 'republican', 'republican', 'democrat', 'democrat',\n",
      "       'democrat', 'democrat', 'democrat', 'republican'], dtype='<U10'), 'Accuracy: 0.7049180327868853')\n"
     ]
    }
   ],
   "source": [
    "### adding features up to 200\n",
    "# %%\n",
    "count_vector = CountVectorizer(stop_words='english', max_features=200)\n",
    "X = count_vector.fit_transform(df.text)\n",
    "labels = count_vector.get_feature_names()\n",
    "# %%\n",
    "# %\n",
    "\n",
    "## takes vectorized tokens, target values, and labels for the \n",
    "def MNB(vector_array, y, labels):\n",
    "    BoW = pd.DataFrame(vector_array.toarray(), columns=labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(BoW, y)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return (clf.predict(X_test), f'Accuracy: {clf.score(X_test, y_test)}')\n",
    "\n",
    "print(MNB(X, df.party, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding features up to 200\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# %%\n",
    "count_vector = CountVectorizer(stop_words='english', max_features=200, ngram_range = ((1,4)))\n",
    "X = count_vector.fit_transform(df.text)\n",
    "labels = count_vector.get_feature_names()\n",
    "# %%\n",
    "BoW = pd.DataFrame(X.toarray(), columns=labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(BoW, df.party)\n",
    "# %\n",
    "\n",
    "## Multinomial Bayes' run\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "MultinomialNB()\n",
    "print(clf.predict(X_test))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['help', 'day', '``', 'believe', 'good', 'job', 'also', 'let',\n",
       "        'great', 'way', 'need', 'last', 'first', 'today', \"''\", 'want',\n",
       "        'many', 'united state', 'applause', \"'ve\", 'congress', 'peace',\n",
       "        'work', 'know', 'right', 'war', 'every', 'united', 'america',\n",
       "        'make', 'think', 'government', 'mr', 'new', \"n't\", 'must', 'time',\n",
       "        'one', 'country', 'state', 'world', 'nation', 'u', 'would',\n",
       "        'president', 'year', 'american', 'people', \"'s\", '--'],\n",
       "       dtype='<U15'),\n",
       " array(['soviet', 'need', 'way', 'today', 'want', 'freedom', 'first',\n",
       "        'say', 'let', 'force', 'well', 'think', 'right', 'every', 'work',\n",
       "        'tax', 'many', '``', 'congress', 'great', 'life', \"'re\", 'make',\n",
       "        'united state', 'war', \"''\", 'know', 'mr', \"'ve\", 'peace',\n",
       "        'united', 'country', 'new', \"n't\", 'time', 'must', 'america',\n",
       "        'one', 'would', 'state', 'nation', 'president', 'government',\n",
       "        'world', 'year', 'u', 'american', 'people', '--', \"'s\"],\n",
       "       dtype='<U15'),\n",
       " ['help', 'day', 'believe', 'good', 'job', 'also', 'last', 'applause'],\n",
       " ['soviet', 'freedom', 'say', 'force', 'well', 'tax', 'life', \"'re\"])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_features(feature_logs, labels, num_features):\n",
    "    ## returns the top featurs for dems, republications, as well as the features that are unique to \n",
    "    #each party in the top num_features\n",
    "    zero_all = np.array(labels)[np.argsort(feature_logs[0])[-1*num_features:]]\n",
    "    one_all = np.array(labels)[np.argsort(feature_logs[1])[-1*num_features:]]\n",
    "    zero_unique = [x for x in zero_all if x not in one_all]\n",
    "    one_unique = [x for x in one_all if x not in zero_all]\n",
    "    return zero_all, one_all, zero_unique, one_unique\n",
    "    \n",
    "    \n",
    "\n",
    "## the most important features for each class after the 200 word, 3 n-gram\n",
    "get_top_features(clf.feature_log_prob_, labels, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top words for democrats{np.array(labels)[last_5[:15]]}')\n",
    "print(f'Top unique words for republicans{np.array(labels)[last_class_1[:15]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenizer from sklearn\n",
    "from nltk import word_tokenize\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding features up to 200, 3-grams, lemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# %%\n",
    "\n",
    "count_vector = CountVectorizer(max_features=200, ngram_range = ((1,4)), tokenizer=WordNetLemmatizer())\n",
    "X = count_vector.fit_transform(df.text)\n",
    "labels = count_vector.get_feature_names()\n",
    "# %%\n",
    "BoW = pd.DataFrame(X.toarray(), columns=labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(BoW, df.party)\n",
    "# %\n",
    "\n",
    "## Multinomial Bayes' run\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "MultinomialNB()\n",
    "print(clf.predict(X_test))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['republican' 'republican' 'democrat' 'democrat' 'democrat' 'republican'\n",
      " 'republican' 'democrat' 'democrat' 'democrat' 'democrat' 'republican'\n",
      " 'republican' 'republican' 'republican' 'republican' 'republican'\n",
      " 'republican' 'republican' 'republican' 'republican' 'republican'\n",
      " 'republican' 'democrat' 'democrat' 'republican' 'republican' 'republican'\n",
      " 'republican' 'republican' 'democrat' 'democrat' 'democrat' 'democrat'\n",
      " 'democrat' 'republican' 'democrat' 'republican' 'republican' 'democrat'\n",
      " 'republican' 'democrat' 'republican' 'democrat' 'republican' 'republican'\n",
      " 'democrat' 'republican' 'republican' 'democrat' 'democrat' 'democrat'\n",
      " 'republican' 'democrat' 'republican' 'republican' 'democrat' 'republican'\n",
      " 'republican' 'republican' 'republican' 'republican' 'democrat' 'democrat'\n",
      " 'republican' 'republican' 'republican' 'republican' 'democrat' 'democrat'\n",
      " 'republican' 'democrat' 'republican' 'democrat' 'republican' 'republican'\n",
      " 'republican' 'democrat' 'democrat' 'democrat' 'democrat' 'republican'\n",
      " 'democrat' 'republican' 'republican' 'republican' 'republican' 'democrat'\n",
      " 'democrat' 'republican' 'democrat' 'democrat' 'republican' 'republican'\n",
      " 'republican' 'republican' 'democrat' 'democrat' 'republican' 'republican'\n",
      " 'democrat' 'republican' 'republican' 'republican' 'republican'\n",
      " 'republican' 'republican' 'republican' 'democrat' 'republican'\n",
      " 'republican' 'republican' 'republican' 'democrat' 'democrat' 'republican'\n",
      " 'democrat' 'democrat' 'republican' 'democrat' 'republican' 'republican']\n",
      "0.6557377049180327\n"
     ]
    }
   ],
   "source": [
    "## lemmatized\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "sw = stopwords.words('english')\n",
    "pt = string.punctuation\n",
    "\n",
    "def doc_to_tokens(doc,ngs=1):\n",
    "    # lower case all words\n",
    "    doc = doc.lower()\n",
    "    # remove accents\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "    # remove odd punctuation\n",
    "    doc = re.sub(r'[~*.^]*', '', doc)\n",
    "    # turn document into a list of tokens\n",
    "    doc = word_tokenize(doc)\n",
    "    # removing stopwords (sw) and punctation (pt)\n",
    "    doc = [token for token in doc if token not in sw and token not in pt]\n",
    "    # lemmatize all tokens\n",
    "    doc = [lem.lemmatize(token) for token in doc]\n",
    "    # here we are setting up the bigrams if specified\n",
    "    ng = list(map(lambda tup: '-'.join(tup), ngrams(doc, ngs)))\n",
    "    return doc + ng\n",
    "\n",
    "token_array = df['text'].apply(doc_to_tokens)\n",
    "\n",
    "## stores tokens in dictionary\n",
    "custom_tokens = {}\n",
    "for i,v in enumerate(token_array.to_numpy()):\n",
    "    custom_tokens[i] = v\n",
    "\n",
    "## initializes CV with given tokens and runs MNB prediction\n",
    "CV = CountVectorizer(\n",
    "    # so we can pass it strings\n",
    "    input='content',\n",
    "    # turn off preprocessing of strings to avoid corrupting our keys\n",
    "    lowercase=False,\n",
    "    preprocessor=lambda x: x,\n",
    "    max_features = 200,\n",
    "    ngram_range = ((1,3)),\n",
    "    # use our token dictionary\n",
    "    tokenizer=lambda key: custom_tokens[key])\n",
    "\n",
    "X = CV.fit_transform(custom_tokens.keys())\n",
    "labels = CV.get_feature_names()\n",
    "# %%\n",
    "BoW = pd.DataFrame(X.toarray(), columns=labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(BoW, df.party)\n",
    "# %\n",
    "\n",
    "## Multinomial Bayes' run\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "MultinomialNB()\n",
    "print(clf.predict(X_test))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero: ['help' 'day' '``' 'believe' 'good' 'job' 'also' 'let' 'great' 'way'\n",
      " 'need' 'last' 'first' 'today' \"''\" 'want' 'many' 'united state'\n",
      " 'applause' \"'ve\" 'congress' 'peace' 'work' 'know' 'right' 'war' 'every'\n",
      " 'united' 'america' 'make' 'think' 'government' 'mr' 'new' \"n't\" 'must'\n",
      " 'time' 'one' 'country' 'state' 'world' 'nation' 'u' 'would' 'president'\n",
      " 'year' 'american' 'people' \"'s\" '--']\n",
      "one: ['soviet' 'need' 'way' 'today' 'want' 'freedom' 'first' 'say' 'let'\n",
      " 'force' 'well' 'think' 'right' 'every' 'work' 'tax' 'many' '``'\n",
      " 'congress' 'great' 'life' \"'re\" 'make' 'united state' 'war' \"''\" 'know'\n",
      " 'mr' \"'ve\" 'peace' 'united' 'country' 'new' \"n't\" 'time' 'must' 'america'\n",
      " 'one' 'would' 'state' 'nation' 'president' 'government' 'world' 'year'\n",
      " 'u' 'american' 'people' '--' \"'s\"]\n",
      "zero unique: ['help' 'day' '``' 'believe' 'good' 'job' 'also' 'let' 'great' 'way'\n",
      " 'need' 'last' 'first' 'today' \"''\" 'want' 'many' 'united state'\n",
      " 'applause' \"'ve\" 'congress' 'peace' 'work' 'know' 'right' 'war' 'every'\n",
      " 'united' 'america' 'make' 'think' 'government' 'mr' 'new' \"n't\" 'must'\n",
      " 'time' 'one' 'country' 'state' 'world' 'nation' 'u' 'would' 'president'\n",
      " 'year' 'american' 'people' \"'s\" '--']\n",
      "one unique: ['soviet' 'need' 'way' 'today' 'want' 'freedom' 'first' 'say' 'let'\n",
      " 'force' 'well' 'think' 'right' 'every' 'work' 'tax' 'many' '``'\n",
      " 'congress' 'great' 'life' \"'re\" 'make' 'united state' 'war' \"''\" 'know'\n",
      " 'mr' \"'ve\" 'peace' 'united' 'country' 'new' \"n't\" 'time' 'must' 'america'\n",
      " 'one' 'would' 'state' 'nation' 'president' 'government' 'world' 'year'\n",
      " 'u' 'american' 'people' '--' \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "zero, one, zero_uniqe, one_unique = get_top_features(clf.feature_log_prob_, labels, 50)\n",
    "\n",
    "print(f'zero: {zero}')\n",
    "print(f'one: {one}')\n",
    "print(f'zero unique: {zero}')\n",
    "print(f'one unique: {one}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop = sw\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sw = ['also', 'day','say', 'let', 've','year', '--', '``', \"'s\", \"''\", 'way', 'would',\"n't\", 'must','u','many','one','first','last',\"'re'\", 'today','']\n",
    "sw.append(new_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['republican' 'democrat' 'democrat' 'democrat' 'democrat' 'democrat'\n",
      " 'democrat' 'democrat' 'democrat' 'republican' 'republican' 'republican'\n",
      " 'republican' 'republican' 'republican' 'democrat' 'democrat' 'democrat'\n",
      " 'democrat' 'republican' 'republican' 'republican' 'democrat' 'republican'\n",
      " 'democrat' 'democrat' 'democrat' 'democrat' 'republican' 'democrat'\n",
      " 'democrat' 'democrat' 'republican' 'republican' 'democrat' 'democrat'\n",
      " 'democrat' 'republican' 'democrat' 'democrat' 'democrat' 'republican'\n",
      " 'democrat' 'democrat' 'republican' 'democrat' 'democrat' 'democrat'\n",
      " 'democrat' 'democrat' 'democrat' 'democrat' 'republican' 'democrat'\n",
      " 'democrat' 'republican' 'republican' 'republican' 'republican' 'democrat'\n",
      " 'republican' 'democrat' 'republican' 'democrat' 'republican' 'democrat'\n",
      " 'republican' 'democrat' 'democrat' 'democrat' 'democrat' 'republican'\n",
      " 'republican' 'republican' 'democrat' 'republican' 'republican'\n",
      " 'republican' 'democrat' 'democrat' 'democrat' 'democrat' 'democrat'\n",
      " 'republican' 'democrat' 'republican' 'democrat' 'democrat' 'democrat'\n",
      " 'republican' 'republican' 'democrat' 'republican' 'democrat' 'republican'\n",
      " 'republican' 'republican' 'republican' 'democrat' 'republican' 'democrat'\n",
      " 'democrat' 'republican' 'republican' 'republican' 'republican' 'democrat'\n",
      " 'democrat' 'democrat' 'republican' 'republican' 'republican' 'democrat'\n",
      " 'republican' 'democrat' 'democrat' 'democrat' 'republican' 'democrat'\n",
      " 'republican' 'republican' 'republican']\n",
      "0.7049180327868853\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "pt = string.punctuation\n",
    "\n",
    "def doc_to_tokens(doc,ngs=1):\n",
    "    # lower case all words\n",
    "    doc = doc.lower()\n",
    "    # remove accents\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "    # remove odd punctuation\n",
    "    doc = re.sub(r'[~*.^]*', '', doc)\n",
    "    # turn document into a list of tokens\n",
    "    doc = word_tokenize(doc)\n",
    "    # removing stopwords (sw) and punctation (pt)\n",
    "    doc = [token for token in doc if token not in sw and token not in pt]\n",
    "    # lemmatize all tokens\n",
    "    doc = [lem.lemmatize(token) for token in doc]\n",
    "    # here we are setting up the bigrams if specified\n",
    "    ng = list(map(lambda tup: '-'.join(tup), ngrams(doc, ngs)))\n",
    "    return doc + ng\n",
    "\n",
    "token_array = df['text'].apply(doc_to_tokens)\n",
    "\n",
    "## stores tokens in dictionary\n",
    "custom_tokens = {}\n",
    "for i,v in enumerate(token_array.to_numpy()):\n",
    "    custom_tokens[i] = v\n",
    "\n",
    "## initializes CV with given tokens and runs MNB prediction\n",
    "CV = CountVectorizer(\n",
    "    # so we can pass it strings\n",
    "    input='content',\n",
    "    # turn off preprocessing of strings to avoid corrupting our keys\n",
    "    lowercase=False,\n",
    "    preprocessor=lambda x: x,\n",
    "    max_features = 200,\n",
    "    ngram_range = ((1,3)),\n",
    "    # use our token dictionary\n",
    "    tokenizer=lambda key: custom_tokens[key])\n",
    "\n",
    "X = CV.fit_transform(custom_tokens.keys())\n",
    "labels = CV.get_feature_names()\n",
    "# %%\n",
    "BoW = pd.DataFrame(X.toarray(), columns=labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(BoW, df.party)\n",
    "# %\n",
    "\n",
    "## Multinomial Bayes' run\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "MultinomialNB()\n",
    "print(clf.predict(X_test))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
